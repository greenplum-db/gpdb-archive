-- start_matchsubs
-- m/\(cost=.*\)/
-- s/\(cost=.*\)//
--
-- m/\(slice\d+; segments: \d+\)/
-- s/\(slice\d+; segments: \d+\)//
-- end_matchsubs
create schema bfv_statistic;
set search_path=bfv_statistic;
create table bfv_statistics_foo (a int, b int) distributed by (a);
insert into bfv_statistics_foo values (1,1);
insert into bfv_statistics_foo values (0,1);
insert into bfv_statistics_foo values (2,1);
insert into bfv_statistics_foo values (null,1);
analyze bfv_statistics_foo;
-- current statistics
select stanullfrac, stadistinct, stanumbers1 from pg_statistic where starelid='bfv_statistics_foo'::regclass and staattnum=1;
 stanullfrac | stadistinct | stanumbers1 
-------------+-------------+-------------
        0.25 |       -0.75 | 
(1 row)

-- exercise GPORCA translator
explain select * from bfv_statistics_foo where a is not null and b >= 1;
                                 QUERY PLAN                                 
----------------------------------------------------------------------------
 Gather Motion 3:1  (slice1; segments: 3)  (cost=0.00..2.05 rows=3 width=8)
   ->  Seq Scan on bfv_statistics_foo  (cost=0.00..2.05 rows=1 width=8)
         Filter: a IS NOT NULL AND b >= 1
 Optimizer: Postgres query optimizer
(4 rows)

create table bfv_statistics_foo2(a int) distributed by (a);
insert into bfv_statistics_foo2 select generate_series(1,5);
insert into bfv_statistics_foo2 select 1 from generate_series(1,5);
insert into bfv_statistics_foo2 select 2 from generate_series(1,4);
insert into bfv_statistics_foo2 select 3 from generate_series(1,3);
insert into bfv_statistics_foo2 select 4 from generate_series(1,2);
insert into bfv_statistics_foo2 select 5 from generate_series(1,1);
analyze bfv_statistics_foo2;
-- current stats
select stanumbers1, stavalues1 from pg_statistic where starelid='bfv_statistics_foo2'::regclass;
       stanumbers1       | stavalues1  
-------------------------+-------------
 {0.3,0.25,0.2,0.15,0.1} | {1,2,3,4,5}
(1 row)

explain select a from bfv_statistics_foo2 where a > 1 order by a;
                                 QUERY PLAN                                  
-----------------------------------------------------------------------------
 Gather Motion 3:1  (slice1; segments: 3)  (cost=2.52..2.55 rows=14 width=4)
   Merge Key: a
   ->  Sort  (cost=2.52..2.55 rows=5 width=4)
         Sort Key: a
         ->  Seq Scan on bfv_statistics_foo2  (cost=0.00..2.25 rows=5 width=4)
               Filter: a > 1
 Optimizer: Postgres query optimizer
(7 rows)

-- change stats manually so that MCV and MCF numbers do not match
set allow_system_table_mods=true;
update pg_statistic set stavalues1='{6,3,1,5,4,2}'::int[] where starelid='bfv_statistics_foo2'::regclass;
-- excercise the translator
explain select a from bfv_statistics_foo2 where a > 1 order by a;
                                 QUERY PLAN                                  
-----------------------------------------------------------------------------
 Gather Motion 3:1  (slice1; segments: 3)  (cost=2.57..2.61 rows=17 width=4)
   Merge Key: a
   ->  Sort  (cost=2.57..2.61 rows=6 width=4)
         Sort Key: a
         ->  Seq Scan on bfv_statistics_foo2  (cost=0.00..2.25 rows=6 width=4)
               Filter: a > 1
 Optimizer: Postgres query optimizer
(7 rows)

--
-- test missing statistics
--
set gp_create_table_random_default_distribution=off;
create table bfv_statistics_foo3(a int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'a' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
select * from gp_toolkit.gp_stats_missing where smischema = 'bfv_statistic' AND  smitable = 'bfv_statistics_foo3';
   smischema   |      smitable       | smisize | smicols | smirecs 
---------------+---------------------+---------+---------+---------
 bfv_statistic | bfv_statistics_foo3 | f       |       1 |       0
(1 row)

--
-- for Orca's Split Operator ensure that the columns needed for stats derivation is correct
--
set gp_create_table_random_default_distribution=off;
CREATE TABLE bar_dml (
    vtrg character varying(6) NOT NULL,
    tec_schuld_whg character varying(3) NOT NULL,
    inv character varying(11) NOT NULL,
    zed_id character varying(6) NOT NULL,
    mkl_id character varying(6) NOT NULL,
    zj integer NOT NULL,
    folio integer NOT NULL,
    zhlg_typ character varying(1) NOT NULL,
    zhlg character varying(8) NOT NULL,
    ant_zhlg double precision,
    zuordn_sys_dat character varying(11),
    zhlg_whg_bilkurs numeric(15,8),
    tec_whg_bilkurs numeric(15,8),
    zhlg_ziel_id character varying(1) NOT NULL,
    btg_tec_whg_gesh numeric(13,2),
    btg_tec_whg_makl numeric(13,2),
    btg_zhlg_whg numeric(13,2),
    zhlg_typ_org character varying(1),
    zhlg_org character varying(8),
    upd_dat date
)
WITH (appendonly=true) DISTRIBUTED RANDOMLY;
update bar_dml set (zhlg_org, zhlg_typ_org) = (zhlg, zhlg_typ);
--
-- Cardinality estimation when there is no histogram and MCV
--
create table bfv_statistics_foo4 (a int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'a' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
insert into bfv_statistics_foo4 select i from generate_series(1,99) i;
insert into bfv_statistics_foo4 values (NULL);
analyze bfv_statistics_foo4;
select stanullfrac, stadistinct, stanumbers1 from pg_statistic where starelid='bfv_statistics_foo4'::regclass and staattnum=1;
 stanullfrac | stadistinct | stanumbers1 
-------------+-------------+-------------
        0.01 |       -0.99 | 
(1 row)

explain select a from bfv_statistics_foo4 where a > 888;
                                 QUERY PLAN                                 
----------------------------------------------------------------------------
 Gather Motion 3:1  (slice1; segments: 3)  (cost=0.00..4.25 rows=1 width=4)
   ->  Seq Scan on bfv_statistics_foo4  (cost=0.00..4.25 rows=1 width=4)
         Filter: a > 888
 Optimizer: Postgres query optimizer
(4 rows)

--
-- Testing that the merging of memo groups inside Orca does not crash cardinality estimation inside Orca
--
create table t1(c1 int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'c1' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
insert into t1 values(1);
select v from (select max(c1) as v, 1 as r from t1 union select 1 as v, 2 as r ) as bfv_statistics_foo group by v;
 v 
---
 1
(1 row)

select v from (select max(c1) as v, 1 as r from t1 union all select 1 as v, 2 as r ) as bfv_statistics_foo group by v;
 v 
---
 1
(1 row)

select v from (select max(c1) as v, 1 as r from t1 union select 1 as v, 2 as r ) as bfv_statistics_foo;
 v 
---
 1
 1
(2 rows)

--
-- test the generation of histogram boundaries for numeric and real data types
--
create table bfv_statistics_foo_real (a int4, b real) distributed randomly;
insert into bfv_statistics_foo_real values (0, 'Infinity');
insert into bfv_statistics_foo_real values (0, '-Infinity');
insert into bfv_statistics_foo_real values (0, 'NaN');
insert into bfv_statistics_foo_real values (0, 'Infinity');
insert into bfv_statistics_foo_real values (0, '-Infinity');
insert into bfv_statistics_foo_real values (0, 'NaN');
insert into bfv_statistics_foo_real values (0, 'Infinity');
insert into bfv_statistics_foo_real values (0, '-Infinity');
insert into bfv_statistics_foo_real values (0, 'NaN');
insert into bfv_statistics_foo_real values (0, 'Infinity');
insert into bfv_statistics_foo_real values (0, '-Infinity');
insert into bfv_statistics_foo_real values (0, 'NaN');
INSERT INTO bfv_statistics_foo_real VALUES (0, '0');
INSERT INTO bfv_statistics_foo_real VALUES (1, '0');
INSERT INTO bfv_statistics_foo_real VALUES (2, '-34338492.215397047');
INSERT INTO bfv_statistics_foo_real VALUES (3, '4.31');
INSERT INTO bfv_statistics_foo_real VALUES (4, '7799461.4119');
INSERT INTO bfv_statistics_foo_real VALUES (5, '16397.038491');
INSERT INTO bfv_statistics_foo_real VALUES (6, '93901.57763026');
INSERT INTO bfv_statistics_foo_real VALUES (7, '-83028485');
INSERT INTO bfv_statistics_foo_real VALUES (8, '74881');
INSERT INTO bfv_statistics_foo_real VALUES (9, '-24926804.045047420');
INSERT INTO bfv_statistics_foo_real VALUES (0, '0');
INSERT INTO bfv_statistics_foo_real VALUES (1, '0');
INSERT INTO bfv_statistics_foo_real VALUES (2, '-34338492.215397047');
INSERT INTO bfv_statistics_foo_real VALUES (3, '4.31');
INSERT INTO bfv_statistics_foo_real VALUES (4, '7799461.4119');
INSERT INTO bfv_statistics_foo_real VALUES (5, '16397.038491');
INSERT INTO bfv_statistics_foo_real VALUES (6, '93901.57763026');
INSERT INTO bfv_statistics_foo_real VALUES (7, '-83028485');
INSERT INTO bfv_statistics_foo_real VALUES (8, '74881');
INSERT INTO bfv_statistics_foo_real VALUES (9, '-24926804.045047420');
INSERT INTO bfv_statistics_foo_real VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_real VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_real VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_real VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_real VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_real VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_real VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_real VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_real VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_real VALUES (9, '-24926804.045047420');
INSERT INTO bfv_statistics_foo_real VALUES (0, '0');
INSERT INTO bfv_statistics_foo_real VALUES (1, '0');
INSERT INTO bfv_statistics_foo_real VALUES (2, '-34338492.215397047');
INSERT INTO bfv_statistics_foo_real VALUES (3, '4.31');
INSERT INTO bfv_statistics_foo_real VALUES (4, '7799461.4119');
INSERT INTO bfv_statistics_foo_real VALUES (5, '16397.038491');
INSERT INTO bfv_statistics_foo_real VALUES (6, '93901.57763026');
INSERT INTO bfv_statistics_foo_real VALUES (7, '-83028485');
INSERT INTO bfv_statistics_foo_real VALUES (8, '74881');
INSERT INTO bfv_statistics_foo_real VALUES (9, '-24926804.045047420');
ANALYZE bfv_statistics_foo_real;
select histogram_bounds from pg_stats where tablename = 'bfv_statistics_foo_real' and attname = 'b';
 histogram_bounds 
------------------
 
(1 row)

select most_common_vals from pg_stats where tablename = 'bfv_statistics_foo_real' and attname = 'b';
                                                 most_common_vals                                                  
-------------------------------------------------------------------------------------------------------------------
 {0,-Infinity,-2.4926804e+07,Infinity,NaN,-8.302849e+07,-3.4338492e+07,4.31,16397.04,74881,93901.58,7.7994615e+06}
(1 row)

create table bfv_statistics_foo_numeric (a int4, b numeric) distributed randomly;
insert into bfv_statistics_foo_numeric values (0, 'NaN');
insert into bfv_statistics_foo_numeric values (0, 'NaN');
insert into bfv_statistics_foo_numeric values (0, 'NaN');
insert into bfv_statistics_foo_numeric values (0, 'NaN');
insert into bfv_statistics_foo_numeric values (0, 'NaN');
insert into bfv_statistics_foo_numeric values (0, 'NaN');
insert into bfv_statistics_foo_numeric values (0, 'NaN');
insert into bfv_statistics_foo_numeric values (0, 'NaN');
INSERT INTO bfv_statistics_foo_numeric VALUES (0, '0');
INSERT INTO bfv_statistics_foo_numeric VALUES (1, '0');
INSERT INTO bfv_statistics_foo_numeric VALUES (2, '-34338492.215397047');
INSERT INTO bfv_statistics_foo_numeric VALUES (3, '4.31');
INSERT INTO bfv_statistics_foo_numeric VALUES (4, '7799461.4119');
INSERT INTO bfv_statistics_foo_numeric VALUES (5, '16397.038491');
INSERT INTO bfv_statistics_foo_numeric VALUES (6, '93901.57763026');
INSERT INTO bfv_statistics_foo_numeric VALUES (7, '-83028485');
INSERT INTO bfv_statistics_foo_numeric VALUES (8, '74881');
INSERT INTO bfv_statistics_foo_numeric VALUES (9, '-24926804.045047420');
INSERT INTO bfv_statistics_foo_numeric VALUES (0, '0');
INSERT INTO bfv_statistics_foo_numeric VALUES (1, '0');
INSERT INTO bfv_statistics_foo_numeric VALUES (2, '-34338492.215397047');
INSERT INTO bfv_statistics_foo_numeric VALUES (3, '4.31');
INSERT INTO bfv_statistics_foo_numeric VALUES (4, '7799461.4119');
INSERT INTO bfv_statistics_foo_numeric VALUES (5, '16397.038491');
INSERT INTO bfv_statistics_foo_numeric VALUES (6, '93901.57763026');
INSERT INTO bfv_statistics_foo_numeric VALUES (7, '-83028485');
INSERT INTO bfv_statistics_foo_numeric VALUES (8, '74881');
INSERT INTO bfv_statistics_foo_numeric VALUES (9, '-24926804.045047420');
INSERT INTO bfv_statistics_foo_numeric VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_numeric VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_numeric VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_numeric VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_numeric VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_numeric VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_numeric VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_numeric VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_numeric VALUES (10, NULL);
INSERT INTO bfv_statistics_foo_numeric VALUES (9, '-24926804.045047420');
INSERT INTO bfv_statistics_foo_numeric VALUES (0, '0');
INSERT INTO bfv_statistics_foo_numeric VALUES (1, '0');
INSERT INTO bfv_statistics_foo_numeric VALUES (2, '-34338492.215397047');
INSERT INTO bfv_statistics_foo_numeric VALUES (3, '4.31');
INSERT INTO bfv_statistics_foo_numeric VALUES (4, '7799461.4119');
INSERT INTO bfv_statistics_foo_numeric VALUES (5, '16397.038491');
INSERT INTO bfv_statistics_foo_numeric VALUES (6, '93901.57763026');
INSERT INTO bfv_statistics_foo_numeric VALUES (7, '-83028485');
INSERT INTO bfv_statistics_foo_numeric VALUES (8, '74881');
INSERT INTO bfv_statistics_foo_numeric VALUES (9, '-24926804.045047420');
INSERT INTO bfv_statistics_foo_numeric SELECT i,i FROM generate_series(1,30) i;
ANALYZE bfv_statistics_foo_numeric;
select histogram_bounds from pg_stats where tablename = 'bfv_statistics_foo_numeric' and attname = 'b';
                                  histogram_bounds                                  
------------------------------------------------------------------------------------
 {1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30}
(1 row)

select most_common_vals from pg_stats where tablename = 'bfv_statistics_foo_numeric' and attname = 'b';
                                               most_common_vals                                                
---------------------------------------------------------------------------------------------------------------
 {NaN,0,-24926804.045047420,-83028485,-34338492.215397047,4.31,16397.038491,74881,93901.57763026,7799461.4119}
(1 row)

reset gp_create_table_random_default_distribution;
--
-- Ensure that VACUUM ANALYZE does not result in incorrect statistics
--
CREATE TABLE T25289_T1 (c int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'c' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
INSERT INTO T25289_T1 VALUES (1);
DELETE FROM T25289_T1;
ANALYZE T25289_T1;
--
-- expect NO more notice after customer run VACUUM FULL
-- 
CREATE TABLE T25289_T2 (c int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'c' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
INSERT INTO T25289_T2 VALUES (1);
DELETE FROM T25289_T2;
VACUUM FULL T25289_T2;
ANALYZE T25289_T2;
--
-- expect NO notice during analyze if table doesn't have empty pages
--
CREATE TABLE T25289_T3 (c int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'c' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
INSERT INTO T25289_T3 VALUES (1);
ANALYZE T25289_T3;
--
-- expect NO notice when analyzing append only tables
-- 
CREATE TABLE T25289_T4 (c int, d int)
WITH (APPENDONLY=ON) DISTRIBUTED BY (c)
PARTITION BY RANGE(d) (START(1) END (5) EVERY(1));
ANALYZE T25289_T4;
--
-- expect NO crash when the statistic slot for an attribute is broken
--
CREATE TABLE good_tab(a int, b text);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'a' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
CREATE TABLE test_broken_stats(a int, b text);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'a' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
INSERT INTO test_broken_stats VALUES(1, 'abc'), (2, 'cde'), (3, 'efg'), (3, 'efg'), (3, 'efg'), (1, 'abc'), (2, 'cde'); 
ANALYZE test_broken_stats;
SET allow_system_table_mods=true;
-- Simulate broken stats by changing the data type of MCV slot to a different type than in pg_attribute 
-- start_matchsubs
-- m/ERROR:  invalid .* of type .*, for attribute of type .* \(selfuncs\.c\:\d+\)/
-- s/\(selfuncs\.c:\d+\)//
-- end_matchsubs
-- Broken MCVs
UPDATE pg_statistic SET stavalues1='{1,2,3}'::int[] WHERE starelid ='test_broken_stats'::regclass AND staattnum=2;
SELECT * FROM test_broken_stats t1, good_tab t2 WHERE t1.b = t2.b;
ERROR:  invalid MCV array of type integer, for attribute of type text (selfuncs.c:4700)
-- Broken histogram
UPDATE pg_statistic SET stakind2=2 WHERE starelid ='test_broken_stats'::regclass AND staattnum=2;
UPDATE pg_statistic SET stavalues2='{1,2,3}'::int[] WHERE starelid ='test_broken_stats'::regclass AND staattnum=2 and stakind2=2;
SELECT * FROM test_broken_stats t1, good_tab t2 WHERE t1.b = t2.b;
ERROR:  invalid histogram of type integer, for attribute of type text (selfuncs.c:4661)
RESET allow_system_table_mods;
-- cardinality estimation for join on varchar, text, char and bpchar columns must account for FreqRemaining and NDVRemaining
-- resulting in better cardinality numbers for the joins in orca
-- start_ignore
DROP TABLE IF EXISTS test_join_card1;
NOTICE:  table "test_join_card1" does not exist, skipping
DROP TABLE IF EXISTS test_join_card2;
NOTICE:  table "test_join_card2" does not exist, skipping
-- end_ignore
CREATE TABLE test_join_card1 (a varchar, b varchar);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'a' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
CREATE TABLE test_join_card2 (a varchar, b varchar);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'a' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
CREATE TABLE test_join_card3 (a varchar, b varchar);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'a' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
INSERT INTO test_join_card1 SELECT i::text, i::text FROM generate_series(1, 20000)i;
INSERT INTO test_join_card2 SELECT i::text, NULL FROM generate_series(1, 179)i;
INSERT INTO test_join_card2 SELECT 1::text, 'a' FROM generate_series(1, 5820)i;
INSERT INTO test_join_card3 SELECT i::text, i::text FROM generate_series(1,10000)i;
ANALYZE test_join_card1;
ANALYZE test_join_card2;
ANALYZE test_join_card3;
EXPLAIN SELECT * FROM test_join_card1 t1, test_join_card2 t2, test_join_card3 t3 WHERE t1.b = t2.b and t3.b = t2.b;
                                                       QUERY PLAN                                                       
------------------------------------------------------------------------------------------------------------------------
 Gather Motion 3:1  (slice4; segments: 3)  (cost=1081.62..1521.42 rows=2910 width=22)
   ->  Hash Join  (cost=1081.62..1463.22 rows=970 width=22)
         Hash Cond: ((t3.b)::text = (t1.b)::text)
         ->  Redistribute Motion 3:3  (slice1; segments: 3)  (cost=0.00..315.00 rows=3334 width=8)
               Hash Key: t3.b
               ->  Seq Scan on test_join_card3 t3  (cost=0.00..115.00 rows=3334 width=8)
         ->  Hash  (cost=1008.87..1008.87 rows=1940 width=14)
               ->  Redistribute Motion 3:3  (slice3; segments: 3)  (cost=533.91..1008.87 rows=1940 width=14)
                     Hash Key: t1.b
                     ->  Hash Join  (cost=533.91..892.47 rows=1940 width=14)
                           Hash Cond: ((t1.b)::text = (t2.b)::text)
                           ->  Seq Scan on test_join_card1 t1  (cost=0.00..227.00 rows=6667 width=10)
                           ->  Hash  (cost=308.95..308.95 rows=5999 width=4)
                                 ->  Broadcast Motion 3:3  (slice2; segments: 3)  (cost=0.00..308.95 rows=5999 width=4)
                                       ->  Seq Scan on test_join_card2 t2  (cost=0.00..68.99 rows=2000 width=4)
 Optimizer: Postgres query optimizer
(16 rows)

-- start_ignore
DROP TABLE IF EXISTS test_join_card1;
DROP TABLE IF EXISTS test_join_card2;
-- end_ignore
-- Test if the table pg_statistic has data in segments
DROP TABLE IF EXISTS test_statistic_1;
CREATE TABLE test_statistic_1(a int, b int);
INSERT INTO test_statistic_1 SELECT i, i FROM generate_series(1, 1000)i;
ANALYZE test_statistic_1;
select count(*) from pg_class c, pg_statistic s where c.oid = s.starelid and relname = 'test_statistic_1';
 count 
-------
     2
(1 row)

select count(*) from pg_class c, gp_dist_random('pg_statistic') s where c.oid = s.starelid and relname = 'test_statistic_1';
 count 
-------
     6
(1 row)

DROP TABLE test_statistic_1;
-- Test that the histogram looks reasonable.
--
-- We once had a bug where the samples gathered from the segments were
-- truncated, leading to highly biased samples.
CREATE TABLE uniformtest(i int4);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'i' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
INSERT INTO uniformtest SELECT g/100 FROM generate_series(0, 9999) g;
BEGIN;
SET LOCAL default_statistics_target=10; -- don't need so many rows for testing
ANALYZE uniformtest;
COMMIT;
-- ANALYZE collects a random sample, so the exact values chosen for the
-- histogram are nondeterministic. But they should be roughly uniformly
-- distributed across the range 0-99. Show some characteristic values.
select case when avg(bound) between 40 and 60 then '40-60' else avg(bound)::text end as avg,
       case when min(bound) <= 5              then '<= 5'  else min(bound)::text end as min,
       case when max(bound) >= 95             then '>= 95' else max(bound)::text end as max
from pg_stats s,
     unnest(histogram_bounds::text::int4[]) as bound
where tablename = 'uniformtest';
  avg  | min  |  max  
-------+------+-------
 40-60 | <= 5 | >= 95
(1 row)

-- ORCA: Test previous scenario with duplicate memo groups running multiple
--       xforms that need stats before applying and reset after applying. It
--       used to be that this scenario could lead to SIGSEGV where stats were
--       reset and were not re-derived between applying the xforms.
SET optimizer_join_order=exhaustive;
SET optimizer_trace_fallback=on;
CREATE TABLE duplicate_memo_group_test_t1 (c11 varchar, c12 integer) DISTRIBUTED BY (c11);
CREATE TABLE duplicate_memo_group_test_t2 (c2 varchar) DISTRIBUTED BY (c2);
CREATE TABLE duplicate_memo_group_test_t3 (c3 varchar) DISTRIBUTED BY (c3);
INSERT INTO duplicate_memo_group_test_t1 SELECT 'something', generate_series(1,900);
INSERT INTO duplicate_memo_group_test_t2 SELECT generate_series(1,900);
ANALYZE duplicate_memo_group_test_t1, duplicate_memo_group_test_t2;
SELECT
     (SELECT c11 FROM duplicate_memo_group_test_t1 WHERE c12 = 100) AS column1,
     (SELECT sum(c12)
FROM duplicate_memo_group_test_t1
     INNER JOIN duplicate_memo_group_test_t2 ON c11 = c2
     INNER JOIN duplicate_memo_group_test_t3 ON c2 = c3
     INNER JOIN duplicate_memo_group_test_t3 a1 on a1.c3 = a2.c3
     LEFT OUTER JOIN duplicate_memo_group_test_t3 a3 ON a1.c3 = a3.c3
     LEFT OUTER JOIN duplicate_memo_group_test_t3 a4 ON a1.c3 = a4.c3
) AS column2
FROM duplicate_memo_group_test_t3 a2;
 column1 | column2 
---------+---------
(0 rows)

-- Tests ORCA coverage for time-related cross-type stats calculation
--
-- Previously, ORCA didn't support stats calculation for time-related
-- cross-type predicates. It used default scale factor for cardinality 
-- estimate, that could sometimes be off by a few orders of magnitude,
-- thence affecting plan performance. This was because date type was
-- converted to int internally, whereas other time-related types were
-- converted to double.
--
-- Using int for date type allows an equality predicate that only 
-- involves the date type to be always viewed as a singleton, rather 
-- than a range in double in ORCA's constraint framework. This provided
-- convenience of implementing stats derivation. However, such choice
-- prevented ORCA from deriving stats from predicates that involve both 
-- date type and other time-related types. Now, in an attempt of
-- supporting cross-type stats calcualtion, we convert date type to 
-- double as well.
--
-- Test filter stats derivation in table scans
drop table if exists t1, t2;
NOTICE:  table "t2" does not exist, skipping
create table t1 (a int, b date);
create table t2 (a int, b date);
insert into t1 select i, j::date from generate_series(1, 10) i, generate_series('2015-01-01','2021-12-31', '1 day'::interval) j;
insert into t2 select i, j::date from generate_series(1, 10) i, generate_series('2021-01-01','2021-12-31', '1 day'::interval) j;
analyze t1, t2;
-- The following two queries should generate the same plan, now that 
-- we support time-related cross-type stats calculation. ORCA should 
-- derive the same stats for t1 (small subset of the total) based on 
-- predicates on t1.b. Prior to this commit, the date-timestamp cross
-- type predicates used in the following queries yielded a cardinality
-- estimate in the order of 3000.
--
-- inequality predicates:
explain select * from t1, t2 where t1.a = t2.a and t1.b < '2015-01-05'::date;
                                     QUERY PLAN                                      
-------------------------------------------------------------------------------------
 Gather Motion 3:1  (slice1; segments: 3)  (cost=116.71..375.05 rows=14349 width=16)
   ->  Hash Join  (cost=116.71..183.74 rows=4783 width=16)
         Hash Cond: (t2.a = t1.a)
         ->  Seq Scan on t2  (cost=0.00..14.17 rows=1217 width=8)
         ->  Hash  (cost=116.54..116.54 rows=13 width=8)
               ->  Seq Scan on t1  (cost=0.00..116.54 rows=13 width=8)
                     Filter: (b < '01-05-2015'::date)
 Optimizer: Postgres-based planner
(8 rows)

explain select * from t1, t2 where t1.a = t2.a and t1.b < '2015-01-05'::timestamp;
                                        QUERY PLAN                                         
-------------------------------------------------------------------------------------------
 Gather Motion 3:1  (slice1; segments: 3)  (cost=116.71..375.05 rows=14349 width=16)
   ->  Hash Join  (cost=116.71..183.74 rows=4783 width=16)
         Hash Cond: (t2.a = t1.a)
         ->  Seq Scan on t2  (cost=0.00..14.17 rows=1217 width=8)
         ->  Hash  (cost=116.54..116.54 rows=13 width=8)
               ->  Seq Scan on t1  (cost=0.00..116.54 rows=13 width=8)
                     Filter: (b < 'Mon Jan 05 00:00:00 2015'::timestamp without time zone)
 Optimizer: Postgres-based planner
(8 rows)

-- equality predicates:
explain select * from t1, t2 where t1.a = t2.a and t1.b = '2015-01-05'::date;
                                     QUERY PLAN                                     
------------------------------------------------------------------------------------
 Gather Motion 3:1  (slice1; segments: 3)  (cost=116.58..195.13 rows=3650 width=16)
   ->  Hash Join  (cost=116.58..146.47 rows=1217 width=16)
         Hash Cond: (t2.a = t1.a)
         ->  Seq Scan on t2  (cost=0.00..14.17 rows=1217 width=8)
         ->  Hash  (cost=116.54..116.54 rows=3 width=8)
               ->  Seq Scan on t1  (cost=0.00..116.54 rows=3 width=8)
                     Filter: (b = '01-05-2015'::date)
 Optimizer: Postgres-based planner
(8 rows)

explain select * from t1, t2 where t1.a = t2.a and t1.b = '2015-01-05'::timestamp;
                                        QUERY PLAN                                         
-------------------------------------------------------------------------------------------
 Gather Motion 3:1  (slice1; segments: 3)  (cost=116.58..195.13 rows=3650 width=16)
   ->  Hash Join  (cost=116.58..146.47 rows=1217 width=16)
         Hash Cond: (t2.a = t1.a)
         ->  Seq Scan on t2  (cost=0.00..14.17 rows=1217 width=8)
         ->  Hash  (cost=116.54..116.54 rows=3 width=8)
               ->  Seq Scan on t1  (cost=0.00..116.54 rows=3 width=8)
                     Filter: (b = 'Mon Jan 05 00:00:00 2015'::timestamp without time zone)
 Optimizer: Postgres-based planner
(8 rows)

-- Test filter stats derivation in dynamic table scans
drop table if exists t1, t2;
create table t1 (a int, b date)
partition by range (b) (
  start (date '2015-01-01') end (date '2021-01-01') every (interval '1' year),
  default partition d);
create table t2 (a int, b date);
insert into t1 select i, j::date from generate_series(1, 10) i, generate_series('2015-01-01','2021-12-31', '1 day'::interval) j;
insert into t2 select i, j::date from generate_series(1, 10) i, generate_series('2015-01-01','2021-12-31', '1 day'::interval) j;
analyze t1, t2;
-- The following two queries should generate the same plan, now that 
-- we support time-related cross-type comparison. ORCA should derive
-- the same stats for t1 (small number of partitions) and t2 (small
-- subset of the total) based on the predicates and allow DPE. Prior
-- to this commit, the date-timestamp cross-type predicates used in
-- the following queries yielded a cardinality estimate in the order
-- of 500~1000. Consequently, the partition selector wasn't propagated.
--
-- inequality predicates (2 out of 7 partitions):
explain select * from t1, t2 where t1.a = t2.a and t1.b = t2.b and t1.b < '2015-01-05'::date;
                                     QUERY PLAN                                     
------------------------------------------------------------------------------------
 Gather Motion 3:1  (slice1; segments: 3)  (cost=34.71..151.40 rows=5 width=16)
   ->  Hash Join  (cost=34.71..151.33 rows=2 width=16)
         Hash Cond: ((t2.a = t1_1_prt_2.a) AND (t2.b = t1_1_prt_2.b))
         ->  Seq Scan on t2  (cost=0.00..116.54 rows=13 width=8)
               Filter: (b < '01-05-2015'::date)
         ->  Hash  (cost=34.49..34.49 rows=14 width=8)
               ->  Append  (cost=0.00..34.49 rows=14 width=8)
                     ->  Seq Scan on t1_1_prt_2  (cost=0.00..17.21 rows=13 width=8)
                           Filter: (b < '01-05-2015'::date)
                     ->  Seq Scan on t1_1_prt_d  (cost=0.00..17.21 rows=1 width=8)
                           Filter: (b < '01-05-2015'::date)
 Optimizer: Postgres-based planner
(12 rows)

explain select * from t1, t2 where t1.a = t2.a and t1.b = t2.b and t1.b < '2015-01-05'::timestamp;
                                           QUERY PLAN                                            
-------------------------------------------------------------------------------------------------
 Gather Motion 3:1  (slice1; segments: 3)  (cost=34.71..151.40 rows=5 width=16)
   ->  Hash Join  (cost=34.71..151.33 rows=2 width=16)
         Hash Cond: ((t2.a = t1_1_prt_2.a) AND (t2.b = t1_1_prt_2.b))
         ->  Seq Scan on t2  (cost=0.00..116.54 rows=13 width=8)
               Filter: (b < 'Mon Jan 05 00:00:00 2015'::timestamp without time zone)
         ->  Hash  (cost=34.49..34.49 rows=14 width=8)
               ->  Append  (cost=0.00..34.49 rows=14 width=8)
                     ->  Seq Scan on t1_1_prt_2  (cost=0.00..17.21 rows=13 width=8)
                           Filter: (b < 'Mon Jan 05 00:00:00 2015'::timestamp without time zone)
                     ->  Seq Scan on t1_1_prt_d  (cost=0.00..17.21 rows=1 width=8)
                           Filter: (b < 'Mon Jan 05 00:00:00 2015'::timestamp without time zone)
 Optimizer: Postgres-based planner
(12 rows)

-- equality predicates (1 out of 7 partitions):
explain select * from t1, t2 where t1.a = t2.a and t1.b = t2.b and t1.b = '2015-01-05'::date;
                                   QUERY PLAN                                    
---------------------------------------------------------------------------------
 Gather Motion 3:1  (slice1; segments: 3)  (cost=17.25..133.97 rows=10 width=16)
   ->  Hash Join  (cost=17.25..133.83 rows=3 width=16)
         Hash Cond: (t2.a = t1_1_prt_2.a)
         ->  Seq Scan on t2  (cost=0.00..116.54 rows=3 width=8)
               Filter: (b = '01-05-2015'::date)
         ->  Hash  (cost=17.21..17.21 rows=3 width=8)
               ->  Seq Scan on t1_1_prt_2  (cost=0.00..17.21 rows=3 width=8)
                     Filter: (b = '01-05-2015'::date)
 Optimizer: Postgres-based planner
(9 rows)

explain select * from t1, t2 where t1.a = t2.a and t1.b = t2.b and t1.b = '2015-01-05'::timestamp;
                                        QUERY PLAN                                         
-------------------------------------------------------------------------------------------
 Gather Motion 3:1  (slice1; segments: 3)  (cost=17.25..133.97 rows=10 width=16)
   ->  Hash Join  (cost=17.25..133.83 rows=3 width=16)
         Hash Cond: (t2.a = t1_1_prt_2.a)
         ->  Seq Scan on t2  (cost=0.00..116.54 rows=3 width=8)
               Filter: (b = 'Mon Jan 05 00:00:00 2015'::timestamp without time zone)
         ->  Hash  (cost=17.21..17.21 rows=3 width=8)
               ->  Seq Scan on t1_1_prt_2  (cost=0.00..17.21 rows=3 width=8)
                     Filter: (b = 'Mon Jan 05 00:00:00 2015'::timestamp without time zone)
 Optimizer: Postgres-based planner
(9 rows)

RESET optimizer_join_order;
RESET optimizer_trace_fallback;
