-- Tests EXPLAIN format output
-- ignore the variable JIT gucs and "optimizer = 'off'" in Settings (unaligned mode + text format)
-- start_matchsubs
-- m/^Settings:.*/
-- s/,?\s*optimizer_jit\w*\s*=\s*[^,\n]+//g
-- m/^Settings:.*/
-- s/,?\s*jit\w*\s*=\s*[^,\n]+//g
-- m/^Settings:.*/
-- s/^Settings:[,\s]*/Settings: /
-- m/^Settings:.*/
-- s/,?\s*optimizer\w*\s*=\s*'off'//g
-- end_matchsubs
-- ignore variable JIT gucs which can be shown when SETTINGS=ON
-- start_matchignore
-- m/^\s+jit\w*:/
-- m/\s*optimizer:\s*"off"/
-- m/^\s+optimizer_jit\w*:/
-- end_matchignore
-- To produce stable regression test output, it's usually necessary to
-- ignore details such as exact costs or row counts.  These filter
-- functions replace changeable output details with fixed strings.
create function explain_filter(text) returns setof text
language plpgsql as
$$
declare
    ln text;
begin
    for ln in execute $1
    loop
        -- Replace any numeric word with just 'N'
        ln := regexp_replace(ln, '-?\m\d+\M', 'N', 'g');
        -- In sort output, the above won't match units-suffixed numbers
        ln := regexp_replace(ln, '\m\d+kB', 'NkB', 'g');
        ln := regexp_replace(ln, '\m\d+K', 'NK', 'g');
        -- Replace slice and segment numbers with 'N'
        ln := regexp_replace(ln, '\mslice\d+', 'sliceN', 'g');
        ln := regexp_replace(ln, '\mseg\d+', 'segN', 'g');
        -- Ignore text-mode buffers output because it varies depending
        -- on the system state
        CONTINUE WHEN (ln ~ ' +Buffers: .*');
        -- Ignore text-mode "Planning:" line because whether it's output
        -- varies depending on the system state
        CONTINUE WHEN (ln = 'Planning:');
        return next ln;
    end loop;
end;
$$;
-- To produce valid JSON output, replace numbers with "0" or "0.0" not "N"
create function explain_filter_to_json(text) returns jsonb
language plpgsql as
$$
declare
    data text := '';
    ln text;
begin
    for ln in execute $1
    loop
        -- Replace any numeric word with just '0'
        ln := regexp_replace(ln, '\m\d+\M', '0', 'g');
        data := data || ln;
    end loop;
    return data::jsonb;
end;
$$;
-- DEFAULT syntax
CREATE TABLE apples(id int PRIMARY KEY, type text);
INSERT INTO apples(id) SELECT generate_series(1, 100000);
CREATE TABLE box_locations(id int PRIMARY KEY, address text);
CREATE TABLE boxes(id int PRIMARY KEY, apple_id int REFERENCES apples(id), location_id int REFERENCES box_locations(id));
WARNING:  referential integrity (FOREIGN KEY) constraints are not supported in Greenplum Database, will not be enforced
WARNING:  referential integrity (FOREIGN KEY) constraints are not supported in Greenplum Database, will not be enforced
--- Check Explain Text format output
select explain_filter('EXPLAIN SELECT * from boxes LEFT JOIN apples ON apples.id = boxes.apple_id LEFT JOIN box_locations ON box_locations.id = boxes.location_id;');
                                                explain_filter                                                
--------------------------------------------------------------------------------------------------------------
 Gather Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N)
   ->  Nested Loop Left Join  (cost=N.N..N.N rows=N width=N)
         Join Filter: true
         ->  Redistribute Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N)
               Hash Key: boxes.apple_id
               ->  Nested Loop Left Join  (cost=N.N..N.N rows=N width=N)
                     Join Filter: true
                     ->  Redistribute Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N)
                           Hash Key: boxes.location_id
                           ->  Seq Scan on boxes  (cost=N.N..N.N rows=N width=N)
                     ->  Index Scan using box_locations_pkey on box_locations  (cost=N.N..N.N rows=N width=N)
                           Index Cond: (id = boxes.location_id)
         ->  Index Scan using apples_pkey on apples  (cost=N.N..N.N rows=N width=N)
               Index Cond: (id = boxes.apple_id)
 Optimizer: GPORCA
(15 rows)

--- Check Explain Analyze Text output that include the slices information
select explain_filter('EXPLAIN (ANALYZE) SELECT * from boxes LEFT JOIN apples ON apples.id = boxes.apple_id LEFT JOIN box_locations ON box_locations.id = boxes.location_id;');
                                                                explain_filter                                                                
----------------------------------------------------------------------------------------------------------------------------------------------
 Gather Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
   ->  Nested Loop Left Join  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
         Join Filter: true
         ->  Redistribute Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
               Hash Key: boxes.apple_id
               ->  Nested Loop Left Join  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
                     Join Filter: true
                     ->  Redistribute Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
                           Hash Key: boxes.location_id
                           ->  Seq Scan on boxes  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
                     ->  Index Scan using box_locations_pkey on box_locations  (cost=N.N..N.N rows=N width=N) (never executed)
                           Index Cond: (id = boxes.location_id)
         ->  Index Scan using apples_pkey on apples  (cost=N.N..N.N rows=N width=N) (never executed)
               Index Cond: (id = boxes.apple_id)
 Optimizer: GPORCA
 Planning Time: N.N ms
   (sliceN)    Executor memory: NK bytes.
   (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).
   (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).
   (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).
 Memory used:  NkB
 Execution Time: N.N ms
(22 rows)

-- Unaligned output format is better for the YAML / XML / JSON outputs.
-- In aligned format, you have end-of-line markers at the end of each line,
-- and its position depends on the longest line. If the width changes, all
-- lines need to be adjusted for the moved end-of-line-marker.
\a
-- YAML Required replaces for costs and time changes
-- Check Explain YAML output
select explain_filter('EXPLAIN (FORMAT YAML) SELECT * from boxes LEFT JOIN apples ON apples.id = boxes.apple_id LEFT JOIN box_locations ON box_locations.id = boxes.location_id;');
explain_filter
- Plan: 
    Node Type: "Gather Motion"
    Senders: N
    Receivers: N
    Slice: N
    Segments: N
    Gang Type: "primary reader"
    Parallel Aware: false
    Startup Cost: N.N
    Total Cost: N.N
    Plan Rows: N
    Plan Width: N
    Plans: 
      - Node Type: "Nested Loop"
        Parent Relationship: "Outer"
        Slice: N
        Segments: N
        Gang Type: "primary reader"
        Parallel Aware: false
        Join Type: "Left"
        Startup Cost: N.N
        Total Cost: N.N
        Plan Rows: N
        Plan Width: N
        Inner Unique: false
        Join Filter: "true"
        Plans: 
          - Node Type: "Redistribute Motion"
            Senders: N
            Receivers: N
            Parent Relationship: "Outer"
            Slice: N
            Segments: N
            Gang Type: "primary reader"
            Parallel Aware: false
            Startup Cost: N.N
            Total Cost: N.N
            Plan Rows: N
            Plan Width: N
            Hash Key: "boxes.apple_id"
            Plans: 
              - Node Type: "Nested Loop"
                Parent Relationship: "Outer"
                Slice: N
                Segments: N
                Gang Type: "primary reader"
                Parallel Aware: false
                Join Type: "Left"
                Startup Cost: N.N
                Total Cost: N.N
                Plan Rows: N
                Plan Width: N
                Inner Unique: false
                Join Filter: "true"
                Plans: 
                  - Node Type: "Redistribute Motion"
                    Senders: N
                    Receivers: N
                    Parent Relationship: "Outer"
                    Slice: N
                    Segments: N
                    Gang Type: "primary reader"
                    Parallel Aware: false
                    Startup Cost: N.N
                    Total Cost: N.N
                    Plan Rows: N
                    Plan Width: N
                    Hash Key: "boxes.location_id"
                    Plans: 
                      - Node Type: "Seq Scan"
                        Parent Relationship: "Outer"
                        Slice: N
                        Segments: N
                        Gang Type: "primary reader"
                        Parallel Aware: false
                        Relation Name: "boxes"
                        Alias: "boxes"
                        Startup Cost: N.N
                        Total Cost: N.N
                        Plan Rows: N
                        Plan Width: N
                  - Node Type: "Index Scan"
                    Parent Relationship: "Inner"
                    Slice: N
                    Segments: N
                    Gang Type: "primary reader"
                    Parallel Aware: false
                    Scan Direction: "Forward"
                    Index Name: "box_locations_pkey"
                    Relation Name: "box_locations"
                    Alias: "box_locations"
                    Startup Cost: N.N
                    Total Cost: N.N
                    Plan Rows: N
                    Plan Width: N
                    Index Cond: "(id = boxes.location_id)"
          - Node Type: "Index Scan"
            Parent Relationship: "Inner"
            Slice: N
            Segments: N
            Gang Type: "primary reader"
            Parallel Aware: false
            Scan Direction: "Forward"
            Index Name: "apples_pkey"
            Relation Name: "apples"
            Alias: "apples"
            Startup Cost: N.N
            Total Cost: N.N
            Plan Rows: N
            Plan Width: N
            Index Cond: "(id = boxes.apple_id)"
  Optimizer: "GPORCA"
(1 row)
SET random_page_cost = 1;
SET cpu_index_tuple_cost = 0.1;
select explain_filter('EXPLAIN (FORMAT YAML, VERBOSE) SELECT * from boxes;');
explain_filter
- Plan: 
    Node Type: "Gather Motion"
    Senders: N
    Receivers: N
    Slice: N
    Segments: N
    Gang Type: "primary reader"
    Parallel Aware: false
    Startup Cost: N.N
    Total Cost: N.N
    Plan Rows: N
    Plan Width: N
    Output: 
      - "id"
      - "apple_id"
      - "location_id"
    Plans: 
      - Node Type: "Seq Scan"
        Parent Relationship: "Outer"
        Slice: N
        Segments: N
        Gang Type: "primary reader"
        Parallel Aware: false
        Relation Name: "boxes"
        Schema: "public"
        Alias: "boxes"
        Startup Cost: N.N
        Total Cost: N.N
        Plan Rows: N
        Plan Width: N
        Output: 
          - "id"
          - "apple_id"
          - "location_id"
  Optimizer: "GPORCA"
  Settings: 
    cpu_index_tuple_cost: "N.N"
    random_page_cost: "N"
(1 row)
select explain_filter('EXPLAIN (FORMAT YAML, VERBOSE, SETTINGS ON) SELECT * from boxes;');
explain_filter
- Plan: 
    Node Type: "Gather Motion"
    Senders: N
    Receivers: N
    Slice: N
    Segments: N
    Gang Type: "primary reader"
    Parallel Aware: false
    Startup Cost: N.N
    Total Cost: N.N
    Plan Rows: N
    Plan Width: N
    Output: 
      - "id"
      - "apple_id"
      - "location_id"
    Plans: 
      - Node Type: "Seq Scan"
        Parent Relationship: "Outer"
        Slice: N
        Segments: N
        Gang Type: "primary reader"
        Parallel Aware: false
        Relation Name: "boxes"
        Schema: "public"
        Alias: "boxes"
        Startup Cost: N.N
        Total Cost: N.N
        Plan Rows: N
        Plan Width: N
        Output: 
          - "id"
          - "apple_id"
          - "location_id"
  Optimizer: "GPORCA"
  Settings: 
    cpu_index_tuple_cost: "N.N"
    random_page_cost: "N"
(1 row)
--- Check Explain Analyze YAML output that include the slices information
select explain_filter('EXPLAIN (ANALYZE, FORMAT YAML) SELECT * from boxes LEFT JOIN apples ON apples.id = boxes.apple_id LEFT JOIN box_locations ON box_locations.id = boxes.location_id;');
explain_filter
- Plan: 
    Node Type: "Gather Motion"
    Senders: N
    Receivers: N
    Slice: N
    Segments: N
    Gang Type: "primary reader"
    Parallel Aware: false
    Startup Cost: N.N
    Total Cost: N.N
    Plan Rows: N
    Plan Width: N
    Actual Startup Time: N.N
    Actual Total Time: N.N
    Actual Rows: N
    Actual Loops: N
    Plans: 
      - Node Type: "Nested Loop"
        Parent Relationship: "Outer"
        Slice: N
        Segments: N
        Gang Type: "primary reader"
        Parallel Aware: false
        Join Type: "Left"
        Startup Cost: N.N
        Total Cost: N.N
        Plan Rows: N
        Plan Width: N
        Actual Startup Time: N.N
        Actual Total Time: N.N
        Actual Rows: N
        Actual Loops: N
        Inner Unique: false
        Join Filter: "true"
        Rows Removed by Join Filter: N
        Plans: 
          - Node Type: "Redistribute Motion"
            Senders: N
            Receivers: N
            Parent Relationship: "Outer"
            Slice: N
            Segments: N
            Gang Type: "primary reader"
            Parallel Aware: false
            Startup Cost: N.N
            Total Cost: N.N
            Plan Rows: N
            Plan Width: N
            Actual Startup Time: N.N
            Actual Total Time: N.N
            Actual Rows: N
            Actual Loops: N
            Hash Key: "boxes.apple_id"
            Plans: 
              - Node Type: "Nested Loop"
                Parent Relationship: "Outer"
                Slice: N
                Segments: N
                Gang Type: "primary reader"
                Parallel Aware: false
                Join Type: "Left"
                Startup Cost: N.N
                Total Cost: N.N
                Plan Rows: N
                Plan Width: N
                Actual Startup Time: N.N
                Actual Total Time: N.N
                Actual Rows: N
                Actual Loops: N
                Inner Unique: false
                Join Filter: "true"
                Rows Removed by Join Filter: N
                Plans: 
                  - Node Type: "Redistribute Motion"
                    Senders: N
                    Receivers: N
                    Parent Relationship: "Outer"
                    Slice: N
                    Segments: N
                    Gang Type: "primary reader"
                    Parallel Aware: false
                    Startup Cost: N.N
                    Total Cost: N.N
                    Plan Rows: N
                    Plan Width: N
                    Actual Startup Time: N.N
                    Actual Total Time: N.N
                    Actual Rows: N
                    Actual Loops: N
                    Hash Key: "boxes.location_id"
                    Plans: 
                      - Node Type: "Seq Scan"
                        Parent Relationship: "Outer"
                        Slice: N
                        Segments: N
                        Gang Type: "primary reader"
                        Parallel Aware: false
                        Relation Name: "boxes"
                        Alias: "boxes"
                        Startup Cost: N.N
                        Total Cost: N.N
                        Plan Rows: N
                        Plan Width: N
                        Actual Startup Time: N.N
                        Actual Total Time: N.N
                        Actual Rows: N
                        Actual Loops: N
                  - Node Type: "Index Scan"
                    Parent Relationship: "Inner"
                    Slice: N
                    Segments: N
                    Gang Type: "primary reader"
                    Parallel Aware: false
                    Scan Direction: "Forward"
                    Index Name: "box_locations_pkey"
                    Relation Name: "box_locations"
                    Alias: "box_locations"
                    Startup Cost: N.N
                    Total Cost: N.N
                    Plan Rows: N
                    Plan Width: N
                    Actual Startup Time: N.N
                    Actual Total Time: N.N
                    Actual Rows: N
                    Actual Loops: N
                    Index Cond: "(id = boxes.location_id)"
                    Rows Removed by Index Recheck: N
          - Node Type: "Index Scan"
            Parent Relationship: "Inner"
            Slice: N
            Segments: N
            Gang Type: "primary reader"
            Parallel Aware: false
            Scan Direction: "Forward"
            Index Name: "apples_pkey"
            Relation Name: "apples"
            Alias: "apples"
            Startup Cost: N.N
            Total Cost: N.N
            Plan Rows: N
            Plan Width: N
            Actual Startup Time: N.N
            Actual Total Time: N.N
            Actual Rows: N
            Actual Loops: N
            Index Cond: "(id = boxes.apple_id)"
            Rows Removed by Index Recheck: N
  Optimizer: "GPORCA"
  Planning Time: N.N
  Triggers: 
  Slice statistics: 
    - Slice: N
      Executor Memory: N
    - Slice: N
      Executor Memory: 
        Average: N
        Workers: N
        Maximum Memory Used: N
    - Slice: N
      Executor Memory: 
        Average: N
        Workers: N
        Maximum Memory Used: N
    - Slice: N
      Executor Memory: 
        Average: N
        Workers: N
        Maximum Memory Used: N
  Statement statistics: 
    Memory used: N
  Execution Time: N.N
(1 row)
--- Check explain analyze sort information in verbose mode
select explain_filter('EXPLAIN (ANALYZE, VERBOSE) SELECT * from boxes ORDER BY apple_id;');
explain_filter
Gather Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
  Output: id, apple_id, location_id
  Merge Key: apple_id
  ->  Sort  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
        Output: id, apple_id, location_id
        Sort Key: boxes.apple_id
        Sort Method:  quicksort  Memory: NkB  Max Memory: NkB  Avg Memory: NkB (N segments)
        Executor Memory: NkB  Segments: N  Max: NkB (segment N)
        work_mem: NkB  Segments: N  Max: NkB (segment N)  Workfile: (N spilling)
        ->  Seq Scan on public.boxes  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
              Output: id, apple_id, location_id
Optimizer: GPORCA
Settings: cpu_index_tuple_cost = 'N.N', random_page_cost = 'N'
Planning Time: N.N ms
  (sliceN)    Executor memory: NK bytes.
  (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).  Work_mem: NK bytes max.
Memory used:  NkB
Execution Time: N.N ms
(18 rows)
RESET random_page_cost;
RESET cpu_index_tuple_cost;
--
-- Test a simple case with JSON and XML output, too.
--
-- This should be enough for those format. The only difference between JSON,
-- XML, and YAML is in the formatting, after all.
-- Check JSON format
select explain_filter_to_json('EXPLAIN (FORMAT JSON, COSTS OFF) SELECT * FROM generate_series(1, 10);');
explain_filter_to_json
[{"Plan": {"Alias": "generate_series", "Slice": 0, "Segments": 0, "Gang Type": "unallocated", "Node Type": "Function Scan", "Function Name": "generate_series", "Parallel Aware": false}, "Optimizer": "GPORCA"}]
(1 row)
select explain_filter('EXPLAIN (FORMAT XML, COSTS OFF) SELECT * FROM generate_series(1, 10);');
explain_filter
<explain xmlns="http://www.postgresql.org/N/explain">
  <Query>
    <Plan>
      <Node-Type>Function Scan</Node-Type>
      <Slice>N</Slice>
      <Segments>N</Segments>
      <Gang-Type>unallocated</Gang-Type>
      <Parallel-Aware>false</Parallel-Aware>
      <Function-Name>generate_series</Function-Name>
      <Alias>generate_series</Alias>
    </Plan>
    <Optimizer>GPORCA</Optimizer>
  </Query>
</explain>
(1 row)
-- Test for an old bug in printing Sequence nodes in JSON/XML format
-- (https://github.com/greenplum-db/gpdb/issues/9410)
CREATE TABLE jsonexplaintest (i int4) PARTITION BY RANGE (i) (START(1) END(3) EVERY(1));
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'i' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
select explain_filter_to_json('EXPLAIN (FORMAT JSON, COSTS OFF) SELECT * FROM jsonexplaintest WHERE i = 2;');
explain_filter_to_json
[{"Plan": {"Plans": [{"Alias": "jsonexplaintest", "Slice": 0, "Filter": "(i = 0)", "Segments": 0, "Gang Type": "primary reader", "Node Type": "Dynamic Seq Scan", "Relation Name": "jsonexplaintest", "Parallel Aware": false, "Parent Relationship": "Outer", "Number of partitions to scan": 0}], "Slice": 0, "Senders": 0, "Segments": 0, "Gang Type": "primary reader", "Node Type": "Gather Motion", "Receivers": 0, "Parallel Aware": false}, "Optimizer": "GPORCA"}]
(1 row)
-- Check Explain Text format output with jit enable
CREATE TABLE jit_explain_output(c1 int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'c1' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
INSERT INTO jit_explain_output SELECT generate_series(1,100);
SET jit = on;
SET jit_above_cost = 0;
SET gp_explain_jit = on;
-- ORCA GUCs to enable JIT
set optimizer_jit_above_cost to 0;
select explain_filter('EXPLAIN SELECT * FROM jit_explain_output LIMIT 10;');
explain_filter
Limit  (cost=N.N..N.N rows=N width=N)
  ->  Gather Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N)
        ->  Seq Scan on jit_explain_output  (cost=N.N..N.N rows=N width=N)
Optimizer: GPORCA
JIT:
  Functions: N
  Options: Inlining false, Optimization false, Expressions true, Deforming true
(7 rows)
-- Check explain anlyze text format output with jit enable
select explain_filter('EXPLAIN (ANALYZE) SELECT * FROM jit_explain_output LIMIT 10;');
explain_filter
Limit  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
  ->  Gather Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
        ->  Seq Scan on jit_explain_output  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
Optimizer: GPORCA
Planning Time: N.N ms
  (sliceN)    Executor memory: NK bytes.
  (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).
Memory used:  NkB
JIT:
  Options: Inlining false, Optimization false, Expressions true, Deforming true.
  (sliceN): Functions: N.N. Timing: N.N ms total.
Execution Time: N.N ms
(12 rows)
-- Check explain analyze json format output with jit enable
select explain_filter_to_json('EXPLAIN (ANALYZE, FORMAT json) SELECT * FROM jit_explain_output LIMIT 10;');
explain_filter_to_json
[{"JIT": {"slice": {"slice": 0, "Timing": 0.0, "functions": 0.0}, "Options": {"Inlining": false, "Deforming": true, "Expressions": true, "Optimization": false}}, "Plan": {"Plans": [{"Plans": [{"Alias": "jit_explain_output", "Slice": 0, "Segments": 0, "Gang Type": "primary reader", "Node Type": "Seq Scan", "Plan Rows": 0, "Plan Width": 0, "Total Cost": 0.0, "Actual Rows": 0, "Actual Loops": 0, "Startup Cost": 0.0, "Relation Name": "jit_explain_output", "Parallel Aware": false, "Actual Total Time": 0.0, "Actual Startup Time": 0.0, "Parent Relationship": "Outer"}], "Slice": 0, "Senders": 0, "Segments": 0, "Gang Type": "primary reader", "Node Type": "Gather Motion", "Plan Rows": 0, "Receivers": 0, "Plan Width": 0, "Total Cost": 0.0, "Actual Rows": 0, "Actual Loops": 0, "Startup Cost": 0.0, "Parallel Aware": false, "Actual Total Time": 0.0, "Actual Startup Time": 0.0, "Parent Relationship": "Outer"}], "Slice": 0, "Segments": 0, "Gang Type": "unallocated", "Node Type": "Limit", "Plan Rows": 0, "Plan Width": 0, "Total Cost": 0.0, "Actual Rows": 0, "Actual Loops": 0, "Startup Cost": 0.0, "Parallel Aware": false, "Actual Total Time": 0.0, "Actual Startup Time": 0.0}, "Triggers": [], "Optimizer": "GPORCA", "Planning Time": 0.0, "Execution Time": 0.0, "Slice statistics": [{"Slice": 0, "Executor Memory": 0}, {"Slice": 0, "Executor Memory": {"Average": 0, "Workers": 0, "Maximum Memory Used": 0}}], "Statement statistics": {"Memory used": 0}}]
(1 row)
RESET jit;
RESET jit_above_cost;
RESET gp_explain_jit;
RESET optimizer_jit_above_cost;
-- Greenplum hash table extra message
CREATE TABLE test_src_tbl AS
SELECT i % 10000 AS a, i % 10000 + 1 AS b FROM generate_series(1, 50000) i DISTRIBUTED BY (a);
ANALYZE test_src_tbl;
-- Enable optimizer_enable_hashagg, and set statement_mem to a small value to force spilling
set optimizer_enable_hashagg = on;
SET statement_mem = '1000kB';
-- Hashagg with spilling
CREATE TABLE test_hashagg_spill AS
SELECT a, COUNT(DISTINCT b) AS b FROM test_src_tbl GROUP BY a;
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause. Creating a NULL policy entry.
select explain_filter('EXPLAIN ANALYZE SELECT a, COUNT(DISTINCT b) AS b FROM test_src_tbl GROUP BY a;');
explain_filter
Gather Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
  ->  HashAggregate  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
        Group Key: a
        Extra Text: (segN)   hash table(s): N; N groups total in N batches, N spill partitions; disk usage: NKB; chain length N.N avg, N max; using N of N buckets; total N expansions.

        ->  HashAggregate  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
              Group Key: a, b
              Planned Partitions: N
              Extra Text: (segN)   hash table(s): N; N groups total in N batches, N spill partitions; disk usage: NKB; chain length N.N avg, N max; using N of N buckets; total N expansions.

              ->  Seq Scan on test_src_tbl  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
Optimizer: GPORCA
Planning Time: N.N ms
  (sliceN)    Executor memory: NK bytes.
* (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).  Work_mem: NK bytes max, NK bytes wanted.
Memory used:  NkB
Memory wanted:  NkB
Execution Time: N.N ms
(18 rows)
-- Hashagg with grouping sets
CREATE TABLE test_hashagg_groupingsets AS
SELECT a, avg(b) AS b FROM test_src_tbl GROUP BY grouping sets ((a), (b));
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause. Creating a NULL policy entry.
-- The planner generates multiple hash tables but ORCA uses Shared Scan.
WITH query_plan (et) AS
(
	SELECT explain_filter(
		'EXPLAIN ANALYZE SELECT a, avg(b) AS b FROM test_src_tbl GROUP BY grouping sets ((a), (b));')
)
SELECT BTRIM(et) as explain_info FROM query_plan WHERE et like '%Extra Text%' limit 2;
explain_info
Extra Text: (segN)   hash table(s): N; N groups total in N batches, N spill partitions; disk usage: NKB; chain length N.N avg, N max; using N of N buckets; total N expansions.
Extra Text: (segN)   hash table(s): N; N groups total in N batches, N spill partitions; disk usage: NKB; chain length N.N avg, N max; using N of N buckets; total N expansions.
(2 rows)
RESET optimizer_enable_hashagg;
RESET statement_mem;
-- Check EXPLAIN format output with BUFFERS enabled
-- Insert rows into a single segment
SET track_io_timing = on;
CREATE TABLE stat_io_timing(a, b) AS SELECT 0, i FROM generate_series(1, 10000) i DISTRIBUTED BY (a);
ANALYZE stat_io_timing;
-- explain_processing_off
select explain_filter('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF)
SELECT a FROM stat_io_timing
WHERE b BETWEEN 5 AND 9;');
explain_filter
Gather Motion N:N  (sliceN; segments: N) (actual time=N.N..N.N rows=N loops=N)
  ->  Seq Scan on stat_io_timing (actual time=N.N..N.N rows=N loops=N)
        Filter: ((b >= N) AND (b <= N))
        Rows Removed by Filter: N
Optimizer: GPORCA
Planning Time: N.N ms
  (sliceN)    Executor memory: NK bytes.
  (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).
Memory used:  NkB
Execution Time: N.N ms
(10 rows)
select explain_filter('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF, SUMMARY OFF)
SELECT a FROM stat_io_timing
WHERE b BETWEEN 5 AND 9;');
explain_filter
Gather Motion N:N  (sliceN; segments: N) (actual time=N.N..N.N rows=N loops=N)
  ->  Seq Scan on stat_io_timing (actual time=N.N..N.N rows=N loops=N)
        Filter: ((b >= N) AND (b <= N))
        Rows Removed by Filter: N
Optimizer: GPORCA
(5 rows)
select explain_filter('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF)
INSERT INTO stat_io_timing (SELECT * FROM stat_io_timing);');
explain_filter
Insert on stat_io_timing (actual time=N.N..N.N rows=N loops=N)
  ->  Seq Scan on stat_io_timing stat_io_timing_1 (actual time=N.N..N.N rows=N loops=N)
Optimizer: GPORCA
Planning Time: N.N ms
  (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).
Memory used:  NkB
Execution Time: N.N ms
(7 rows)
select explain_filter_to_json('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF, FORMAT JSON)
INSERT INTO stat_io_timing (SELECT * FROM stat_io_timing);');
explain_filter_to_json
[{"Plan": {"Alias": "stat_io_timing", "Plans": [{"Alias": "stat_io_timing_1", "Slice": 0, "Segments": 0, "Gang Type": "primary writer", "Node Type": "Seq Scan", "Actual Rows": 0, "Actual Loops": 0, "I/O Read Time": 0.0, "Relation Name": "stat_io_timing", "I/O Write Time": 0.0, "Parallel Aware": false, "Local Hit Blocks": 0, "Temp Read Blocks": 0, "Actual Total Time": 0.0, "Local Read Blocks": 0, "Shared Hit Blocks": 0, "Shared Read Blocks": 0, "Actual Startup Time": 0.0, "Parent Relationship": "Member", "Temp Written Blocks": 0, "Local Dirtied Blocks": 0, "Local Written Blocks": 0, "Shared Dirtied Blocks": 0, "Shared Written Blocks": 0}], "Slice": 0, "Segments": 0, "Gang Type": "primary writer", "Node Type": "ModifyTable", "Operation": "Insert", "Actual Rows": 0, "Actual Loops": 0, "I/O Read Time": 0.0, "Relation Name": "stat_io_timing", "I/O Write Time": 0.0, "Parallel Aware": false, "Local Hit Blocks": 0, "Temp Read Blocks": 0, "Actual Total Time": 0.0, "Local Read Blocks": 0, "Shared Hit Blocks": 0, "Shared Read Blocks": 0, "Actual Startup Time": 0.0, "Temp Written Blocks": 0, "Local Dirtied Blocks": 0, "Local Written Blocks": 0, "Shared Dirtied Blocks": 0, "Shared Written Blocks": 0}, "Triggers": [], "Optimizer": "GPORCA", "Planning Time": 0.0, "Execution Time": 0.0, "Slice statistics": [{"Slice": 0, "Executor Memory": {"Average": 0, "Workers": 0, "Maximum Memory Used": 0}}], "Statement statistics": {"Memory used": 0}}]
(1 row)
select explain_filter('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF, SUMMARY OFF)
INSERT INTO stat_io_timing (SELECT * FROM stat_io_timing);');
explain_filter
Insert on stat_io_timing (actual time=N.N..N.N rows=N loops=N)
  ->  Seq Scan on stat_io_timing stat_io_timing_1 (actual time=N.N..N.N rows=N loops=N)
Optimizer: GPORCA
(3 rows)
select explain_filter_to_json('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF, FORMAT JSON)
SELECT b FROM stat_io_timing where b=50;');
explain_filter_to_json
[{"Plan": {"Plans": [{"Alias": "stat_io_timing", "Slice": 0, "Filter": "(b = 0)", "Segments": 0, "Gang Type": "primary reader", "Node Type": "Seq Scan", "Actual Rows": 0, "Actual Loops": 0, "I/O Read Time": 0.0, "Relation Name": "stat_io_timing", "I/O Write Time": 0.0, "Parallel Aware": false, "Local Hit Blocks": 0, "Temp Read Blocks": 0, "Actual Total Time": 0.0, "Local Read Blocks": 0, "Shared Hit Blocks": 0, "Shared Read Blocks": 0, "Actual Startup Time": 0.0, "Parent Relationship": "Outer", "Temp Written Blocks": 0, "Local Dirtied Blocks": 0, "Local Written Blocks": 0, "Shared Dirtied Blocks": 0, "Shared Written Blocks": 0, "Rows Removed by Filter": 0}], "Slice": 0, "Senders": 0, "Segments": 0, "Gang Type": "primary reader", "Node Type": "Gather Motion", "Receivers": 0, "Actual Rows": 0, "Actual Loops": 0, "I/O Read Time": 0.0, "I/O Write Time": 0.0, "Parallel Aware": false, "Local Hit Blocks": 0, "Temp Read Blocks": 0, "Actual Total Time": 0.0, "Local Read Blocks": 0, "Shared Hit Blocks": 0, "Shared Read Blocks": 0, "Actual Startup Time": 0.0, "Temp Written Blocks": 0, "Local Dirtied Blocks": 0, "Local Written Blocks": 0, "Shared Dirtied Blocks": 0, "Shared Written Blocks": 0}, "Triggers": [], "Optimizer": "GPORCA", "Planning Time": 0.0, "Execution Time": 0.0, "Slice statistics": [{"Slice": 0, "Executor Memory": 0}, {"Slice": 0, "Executor Memory": {"Average": 0, "Workers": 0, "Maximum Memory Used": 0}}], "Statement statistics": {"Memory used": 0}}]
(1 row)
CREATE INDEX stat_io_timing_idx ON stat_io_timing (b);
select explain_filter('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF)
SELECT b FROM stat_io_timing where b=50;');
explain_filter
Gather Motion N:N  (sliceN; segments: N) (actual time=N.N..N.N rows=N loops=N)
  ->  Index Scan using stat_io_timing_idx on stat_io_timing (actual time=N.N..N.N rows=N loops=N)
        Index Cond: (b = N)
        I/O Timings: read=N.N
Optimizer: GPORCA
Planning Time: N.N ms
  (sliceN)    Executor memory: NK bytes.
  (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).
Memory used:  NkB
Execution Time: N.N ms
(10 rows)
select explain_filter_to_json('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF, FORMAT JSON)
SELECT b FROM stat_io_timing where b=50;');
explain_filter_to_json
[{"Plan": {"Plans": [{"Alias": "stat_io_timing", "Slice": 0, "Segments": 0, "Gang Type": "primary reader", "Node Type": "Index Scan", "Index Cond": "(b = 0)", "Index Name": "stat_io_timing_idx", "Actual Rows": 0, "Actual Loops": 0, "I/O Read Time": 0.0, "Relation Name": "stat_io_timing", "I/O Write Time": 0.0, "Parallel Aware": false, "Scan Direction": "Forward", "Local Hit Blocks": 0, "Temp Read Blocks": 0, "Actual Total Time": 0.0, "Local Read Blocks": 0, "Shared Hit Blocks": 0, "Shared Read Blocks": 0, "Actual Startup Time": 0.0, "Parent Relationship": "Outer", "Temp Written Blocks": 0, "Local Dirtied Blocks": 0, "Local Written Blocks": 0, "Shared Dirtied Blocks": 0, "Shared Written Blocks": 0, "Rows Removed by Index Recheck": 0}], "Slice": 0, "Senders": 0, "Segments": 0, "Gang Type": "primary reader", "Node Type": "Gather Motion", "Receivers": 0, "Actual Rows": 0, "Actual Loops": 0, "I/O Read Time": 0.0, "I/O Write Time": 0.0, "Parallel Aware": false, "Local Hit Blocks": 0, "Temp Read Blocks": 0, "Actual Total Time": 0.0, "Local Read Blocks": 0, "Shared Hit Blocks": 0, "Shared Read Blocks": 0, "Actual Startup Time": 0.0, "Temp Written Blocks": 0, "Local Dirtied Blocks": 0, "Local Written Blocks": 0, "Shared Dirtied Blocks": 0, "Shared Written Blocks": 0}, "Triggers": [], "Optimizer": "GPORCA", "Planning Time": 0.0, "Execution Time": 0.0, "Slice statistics": [{"Slice": 0, "Executor Memory": 0}, {"Slice": 0, "Executor Memory": {"Average": 0, "Workers": 0, "Maximum Memory Used": 0}}], "Statement statistics": {"Memory used": 0}}]
(1 row)
select explain_filter('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF)
SELECT s1.b FROM stat_io_timing s1 join stat_io_timing s2 on s1.b=s2.b where s1.a=50;');
explain_filter
Gather Motion N:N  (sliceN; segments: N) (actual time=N.N..N.N rows=N loops=N)
  ->  Nested Loop (actual time=N.N..N.N rows=N loops=N)
        Join Filter: true
        ->  Broadcast Motion N:N  (sliceN; segments: N) (actual time=N.N..N.N rows=N loops=N)
              ->  Seq Scan on stat_io_timing (actual time=N.N..N.N rows=N loops=N)
                    Filter: (a = N)
        ->  Index Scan using stat_io_timing_idx on stat_io_timing stat_io_timing_1 (never executed)
              Index Cond: (b = stat_io_timing.b)
Optimizer: GPORCA
Planning Time: N.N ms
  (sliceN)    Executor memory: NK bytes.
  (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).
  (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).
Memory used:  NkB
Execution Time: N.N ms
(15 rows)
-- Test Bitmap Heap Scan block accounting
SET enable_seqscan = 0;
SET enable_bitmapscan = 1;
SET optimizer_enable_tablescan = 0;
SET optimizer_enable_bitmapscan = 1;
CREATE INDEX stat_io_timing_brin_idx ON stat_io_timing USING brin (b);
VACUUM ANALYZE stat_io_timing;
select explain_filter('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF)
SELECT * FROM stat_io_timing WHERE b = 1;');
explain_filter
Gather Motion N:N  (sliceN; segments: N) (actual time=N.N..N.N rows=N loops=N)
  ->  Index Scan using stat_io_timing_idx on stat_io_timing (actual time=N.N..N.N rows=N loops=N)
        Index Cond: (b = N)
Optimizer: GPORCA
Planning Time: N.N ms
  (sliceN)    Executor memory: NK bytes.
  (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).
Memory used:  NkB
Execution Time: N.N ms
(9 rows)
RESET track_io_timing;
RESET enable_seqscan;
RESET enable_bitmapscan;
RESET optimizer_enable_tablescan;
RESET optimizer_enable_bitmapscan;
-- Cleanup
DROP TABLE boxes;
DROP TABLE apples;
DROP TABLE box_locations;
DROP TABLE jsonexplaintest;
DROP TABLE jit_explain_output;
DROP TABLE test_src_tbl;
DROP TABLE test_hashagg_spill;
DROP TABLE test_hashagg_groupingsets;
DROP TABLE stat_io_timing;
