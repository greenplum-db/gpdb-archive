-- Try to verify that a session fatal due to OOM should have no effect on other sessions.
-- Report on https://github.com/greenplum-db/gpdb/issues/12399

-- Because the number of errors reported to master can depend on ic types (i.e. ic-tcp and ic-proxy have one
-- additional error from the backend on seg0 which is trying to tear down TCP connection), we have to ignore
-- all of them.
-- start_matchignore
-- m/ERROR:  Error on receive from seg0.*\n/
-- m/\tbefore or while.*\n/
-- m/\tThis probably means.*\n/
-- end_matchignore

create extension if not exists gp_inject_fault;
CREATE

1: select gp_inject_fault('make_dispatch_result_error', 'skip', dbid) from gp_segment_configuration where role = 'p' and content = -1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)
2: begin;
BEGIN

-- session1 will be fatal.
1: select count(*) > 0 from gp_dist_random('pg_class');
FATAL:  could not allocate resources for segworker communication (cdbdisp_async.c:319)
server closed the connection unexpectedly

-- session2 should be ok.
2: select count(*) > 0 from gp_dist_random('pg_class');
 ?column? 
----------
 t        
(1 row)
2: commit;
COMMIT
1q: ... <quitting>
2q: ... <quitting>

select gp_inject_fault('make_dispatch_result_error', 'reset', dbid) from gp_segment_configuration where role = 'p' and content = -1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)

--
-- Test case for the WaitEvent of dispatch
-- The specific event will be watched in pg_stat_activity
--

create table test_waitevent(i int);
CREATE
insert into test_waitevent select generate_series(1,1000);
INSERT 1000

1: select gp_inject_fault_infinite('send_qe_details_init_backend', 'suspend', 2);
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)
1&: select count(*) from test_waitevent;  <waiting ...>
-- check Dispatch/Gang-Assign event: wait all gangs created
2: select wait_event from pg_stat_activity where wait_event_type='IPC' and query = 'select count(*) from test_waitevent;';
 wait_event           
----------------------
 Dispatch/Gang-Assign 
(1 row)
2: select gp_wait_until_triggered_fault('send_qe_details_init_backend', 1, 2);
 gp_wait_until_triggered_fault 
-------------------------------
 Success:                      
(1 row)
2: select gp_inject_fault_infinite('send_qe_details_init_backend', 'resume', 2);
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)
2: select gp_inject_fault_infinite('send_qe_details_init_backend', 'reset', 2);
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)
2q: ... <quitting>
1<:  <... completed>
 count 
-------
 1000  
(1 row)
1q: ... <quitting>

1: select gp_inject_fault_infinite('qe_exec_finished', 'suspend', 2);
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)
1&: select count(*) from test_waitevent;  <waiting ...>
-- check Dispatch/Gang-Result event: wait all QEs finished
2: select wait_event from pg_stat_activity where wait_event_type='IPC' and query = 'select count(*) from test_waitevent;';
 wait_event      
-----------------
 Dispatch/Result 
(1 row)
2: select gp_wait_until_triggered_fault('qe_exec_finished', 1, 2);
 gp_wait_until_triggered_fault 
-------------------------------
 Success:                      
(1 row)
2: select gp_inject_fault_infinite('qe_exec_finished', 'resume', 2);
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)
2: select gp_inject_fault_infinite('qe_exec_finished', 'reset', 2);
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)
2q: ... <quitting>
1<:  <... completed>
 count 
-------
 1000  
(1 row)
1q: ... <quitting>

--
-- Test case for the WaitEvent of ShareInputScan
--

1: set optimizer = off;
SET
1: set gp_cte_sharing to on;
SET
1: select gp_inject_fault_infinite('shareinput_writer_notifyready', 'suspend', 2);
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)
1&: WITH a1 as (select * from test_waitevent), a2 as (select * from test_waitevent) SELECT sum(a1.i)  FROM a1 INNER JOIN a2 ON a2.i = a1.i  UNION ALL SELECT count(a1.i)  FROM a1 INNER JOIN a2 ON a2.i = a1.i;  <waiting ...>
-- start_ignore
2: copy (select pg_stat_get_activity(NULL) from gp_dist_random('gp_id') where gp_segment_id=0) to '/tmp/_gpdb_test_output.txt';
COPY 9
-- end_ignore
2: select gp_wait_until_triggered_fault('shareinput_writer_notifyready', 1, 2);
 gp_wait_until_triggered_fault 
-------------------------------
 Success:                      
(1 row)
2: select gp_inject_fault_infinite('shareinput_writer_notifyready', 'resume', 2);
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)
2: select gp_inject_fault_infinite('shareinput_writer_notifyready', 'reset', 2);
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)
2q: ... <quitting>
1<:  <... completed>
 sum    
--------
 500500 
 1000   
(2 rows)
1q: ... <quitting>

!\retcode grep ShareInputScan /tmp/_gpdb_test_output.txt;
-- start_ignore
(100897,9460,10,"",active,"WITH a1 as (select * from test_waitevent), a2 as (select * from test_waitevent) SELECT sum(a1.i)  FROM a1 INNER JOIN a2 ON a2.i = a1.i  UNION ALL SELECT count(a1.i)  FROM a1 INNER JOIN a2 ON a2.i = a1.i;",IPC,ShareInputScan,"Sat Mar 12 23:51:16.151757 2022 PST","Sat Mar 12 23:51:16.151757 2022 PST","Sat Mar 12 23:51:16.14545 2022 PST","Sat Mar 12 23:51:16.151797 2022 PST",127.0.0.1,,63602,,7398,"client backend",f,,,,,,,,f,,f,247,0,unknown)

-- end_ignore
(exited with code 0)

--
-- Test for issue https://github.com/greenplum-db/gpdb/issues/12703
--

-- Case for cdbgang_createGang_async
1: create table t_12703(a int);
CREATE

1:begin;
BEGIN
-- make a cursor so that we have a named portal
1: declare cur12703 cursor for select * from t_12703;
DECLARE

-- next, trigger a segment down so the existing session will be reset
2: select gp_inject_fault('start_prepare', 'panic', dbid) from gp_segment_configuration where role = 'p' AND content = 0;
 gp_inject_fault 
-----------------
 Success:        
(1 row)
2: create table t_12703_2(a int);
ERROR:  fault triggered, fault name:'start_prepare' fault type:'panic'  (seg0 127.0.1.1:7002 pid=18359)

-- this will go to cdbgang_createGang_async's code path
-- for some segments are DOWN. It should not PANIC even
-- with a named portal existing.
1: select * from t_12703;
ERROR:  gang was lost due to cluster reconfiguration (cdbgang_async.c:98)
1: abort;
ABORT

1q: ... <quitting>
2q: ... <quitting>

-- Case for cdbCopyEndInternal
-- Provide some data to copy in
4: insert into t_12703 select * from generate_series(1, 10)i;
INSERT 10
4: copy t_12703 to '/tmp/t_12703';
COPY 10
-- make copy in statement hang at the entry point of cdbCopyEndInternal
4: select gp_inject_fault('cdb_copy_end_internal_start', 'suspend', dbid) from gp_segment_configuration where role = 'p' and content = -1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)
4q: ... <quitting>
1&: copy t_12703 from '/tmp/t_12703';  <waiting ...>
select gp_wait_until_triggered_fault('cdb_copy_end_internal_start', 1, dbid) from gp_segment_configuration where role = 'p' and content = -1;
 gp_wait_until_triggered_fault 
-------------------------------
 Success:                      
(1 row)
-- make Gang connection is BAD
3: select gp_inject_fault('start_prepare', 'panic', dbid) from gp_segment_configuration where role = 'p' AND content = 1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)
3: create table t_12703_2(a int);
ERROR:  fault triggered, fault name:'start_prepare' fault type:'panic'  (seg0 127.0.1.1:7002 pid=18412)
2: begin;
BEGIN
select gp_inject_fault('cdb_copy_end_internal_start', 'reset', dbid) from gp_segment_configuration where role = 'p' and content = -1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)
-- continue copy it should not PANIC
1<:  <... completed>
ERROR:  MPP detected 1 segment failures, system is reconnected
1q: ... <quitting>
-- session 2 still alive (means not PANIC happens)
2: select 1;
 ?column? 
----------
 1        
(1 row)
2: end;
END
2q: ... <quitting>


-- loop while segments come in sync
select wait_until_all_segments_synchronized();
 wait_until_all_segments_synchronized 
--------------------------------------
 OK                                   
(1 row)

-- verify no segment is down after recovery
select count(*) from gp_segment_configuration where status = 'd';
 count 
-------
 0     
(1 row)

